% events:
% tolmach
% lewis?
% boothe00

\chapter{Related Work}
\label{sec:relatedwork}

In this chapter, we present studies and debugging tools related to our contributions.
In \cref{sec:rw_studies}, we examine studies on how developers debug and use debugging tools.
\Cref{sec:rw_bit_debugging} discusses back-in-time debuggers.
\Cref{sec:rw_dynamic_slicing} gives an overview over slicing algorithms.
In \cref{sec:rw_slice_debugging}, we compare debugging tools that use slicing or related techniques to aide developers.
\Cref{sec:rw_system_debugging} and 
\cref{sec:rw_visualization}


\section{Studies}
\label{sec:rw_studies}

Many studies examined how developers approach software maintenance tasks with and without specialized tools.
The requirements we defined for the debugging approaches that form the contribution of this work, as described in \cref{sec:requirements}, "\nameref{sec:requirements}", are based on the outcomes of these studies.
Here, we present a selection of notable studies in our field and their results.

Gould let 10 experienced developers locate bugs in Fortran programs~\cite{gould75:some_psychological_evidence}.
He found that developers can locate bugs up to three times faster when they were familiar with the code.\todo{...}
Furthermore, developers were reluctant to use interactive debugging tools as along as they believed they could find the bug by just reading the code.

Gugerty and Olson studied the difference between novice and expert programmers~\cite{gugerty86:comprehension_differences_in_debugging}.
In the study, both experts and novices used the same strategies to approach fault localization.
However, experts showed superior skills at program comprehension, as they not only required less time needed to form initial hypotheses, but also had initial hypotheses of significantly higher quality.
The quality of program understanding strongly correlated with the quality of fixes.
Novices not only took much longer to develop a fix, they also often introduced additional bugs in the process.

Storey \etal explored the question of how program understanding tools change the way developers approach program comprehension~\cite{storey97:how_do_program_understanding}.
Three software exploration tools that provide visual abstractions were used to solve high-level program understanding tasks.
The study found that programmers approach program comprehension with a variety of strategies and tools were most effective if they supported the developer's preferred strategy instead of imposing a different approach.

In a follow-up study, Storey \etal classified program comprehension strategies and design elements for supporting tools~\cite{storey99:cognitive_design_elements}.
Experienced developers often used a hypothesis-driven top-down approach, but relied on bottom-up strategies to identify abstractions.
In some cases, developers sought to fill only specific gaps in their knowledge of the program, increasing their understanding as needed to locate a bug.
Meta-approaches combine multiple strategies into adaptable tools.
Independently of the strategy, Storey \etal found that program comprehension tools should reduce developers' cognitive overhead by supporting easy navigation and providing orientation in the program.

In two studies, Sillito \etal observed developers working on change tasks to identify their information needs~\cite{sillito06:questions_programmers_ask}.
From this, 44 questions developers frequently ask were derived.
These questions were divided into four categories:
Questions to "find an initial focus point" are most often asked by newcomers, searching for a "place to start looking".
"Building on those points," developers ask questions to understand code in its immediate context.
As the program understanding grows, developers ask questions about sub-graphs, seeking to understand the behavior and purpose of modules.
Finally, developers ask questions over groups of sub-graphs, to understand how different modules interact or relate to each other.
The second and third category contain many questions about program behavior and control flow that can not be easily answered with traditional debugging tools.

While previous studies only look at individual developers, Ko \etal studied the information needs of developers working in collocated teams~\cite{ko07:information_needs_in_collocated}.
By transcribing work sessions minute by minute, they identified 21 information needs.
Developers often collaborated to find answers to their questions, relying on their coworkers knowledge but also causing interruptions.
Ko \etal found that for many questions, better tool support could reduce the time and overhead needed to find an answer.
In other cases, using better tools can enable better collaboration; for instance, post-mortem debuggers allow sharing a debug session on multiple computers.

Weiser found that common approaches to code modularization often do not properly reflect how developers understand a program~\cite{weiser82:programmers_use_slices_when}.
While code is often grouped in terms of functional relation, developers often prefer to understand code in sets of statements related by control flow, not matching the file or module structure.
In the absence of aspect-oriented modularization, slicing techniques can identify such related sets of statements and thereby support program comprehension.

Johnson \etal interviewed developers to study the usage of software analysis tools~\cite{johnson13:why_dont_software_developers}.
While the study focused on static tools, many of the insights can be transferred to dynamic analysis tools as well.
In the study, all developers reported the examined analysis tools to be useful, but were mostly reluctant to use them on a regular basis, due to multiple barriers.
First, false positives and difficult-to-understand results imposed a high cognitive overhead on developers.
Furthermore, a lack of integration into the regular development workflow (and into the IDE in particular) combined with slow responsiveness caused tools to be more of an interruption than actual help.
This was exacerbated by a lack of customizability to the developers' needs.
Finally, tools that did not support collaboration with coworkers were not adopted by the team as a whole, reducing the usefulness for each individual.

Perscheid \etal studied the adoption of advanced debugging techniques by observing professional software developers and conducting an online survey~\cite{perscheid17:studying_the_advancement}.
They found that while most developers are proficient using a symbolic debuggers, knowledge of more sophisticated debugging techniques is sparse.
Furthermore, developers in the study reported a long distance between observed failure and root cause as the main difficulty of finding bugs and wished for more easily accessible debugging tools.

\section{Back-in-Time Debugging}
\label{sec:rw_bit_debugging}

%A symbolic debugger allows developers to inspect the program state at single points in time only.
%Having access to a program's execution history can be beneficial to developers in multiple ways, for example to run analyses on the execution or to follow an infection chain backwards through time.

Reversing a program execution was for the first time made possible with the EXDAMS debugging system~\cite{balzer69:exdams_extendable_debugging}.
Since then, many debuggers that use execution history or back-in-time operations have been developed and various approaches with different advantages and draw-backs were tested.

Powell and Linton developed a debugging environment that stores a program's code, state, and execution history in a relational database~\cite{powell83:a_database_model}.
Using this system, developers can express questions about program behavior as database queries.
This allows developers to formulate complex questions with high precision.

IGOR is a snapshot-based back-in-time debugger by Feldman and Brown~\cite{feldman88:igor_a_system}.
By delegating the creation and management of snapshots to the operating system, the performance overhead is kept low.
IGOR supports reversing execution, searching the execution history, and substituting data and program parts during execution.

Tolmach and Appel developed a snapshot-based back-in-time debugger that operates at the source level~\cite{tolmach93:a_debugger_for_standard}.
The program code is pre-processed before compilation to facilitate back-in-time debugging without the need for changes to the compiler or runtime environment.
Creating snapshots as first-class continuations reduces the complexity of the debugging system by reusing respective interpreter functionality to replace execution.
%creates checkpoints as first-class continuations. This
%allows a flexible mechanism to replace execution and to provide reverse execution for
%the user and the interpreter.

ZStep95 is a back-in-time debugger for LISP by Lieberman and Fry that keeps a full execution history to reverse control flow~\cite{lieberman95:zstep_95_a_reversible}.
This way, individual instructions can be reversed faster than with a snapshot-based approach.
Furthermore, ZStep95 records the screen to include the user interface in the debug session.
Developers can jump to the previous or next UI change, debugging a program by its observable behavior.

Boothe developed a snapshot-based back-in-time debugger for C and C++~\cite{boothe00:efficient_algorithms_for_bidirectional}.
I/O logging ensures the deterministic re-execution between checkpoints and a sequential numbering of events allows the debugger to identify events across multiple re-executions.
Memory overhead is reduced through exponential checkpoint thinning, based on the idea that developers are more likely to reverse execution in small steps and that longer waiting times for larger steps are acceptable.

Cook developed a semantic model for the reverse execution of stack-based bytecode languages~\cite{cook02:reverse_execution_of_java}.
A prototype debugger using this model was implemented in a Java VM.
The model retains information otherwise lost during state-changing operations.
This allows the debugger to execute the program backwards, but not to jump to previous points in time or to search the execution history.

In 2003, Lewis presented the Omniscient Debugger for Java~\cite{lewis03:debugging_backwards_in_time}.
With an omniscient debugger, the entire execution history is searchable and state from any point-in-time can be restored in constant time.
The latter is achieved using a post-mortem approach.
Instead of rewinding the actual program, the debugger only presents the program state, as it was, in its user interface.

UNSTUCK is an omniscient debugger for Smalltalk by Hofer \etal~\cite{hofer06:design_and_implementation}.
It is directly integrated in the IDE and features searching and code highlighting.

To better support the large execution traces required by omniscient debuggers, Pothier \etal developed a distributed database to store and search traces~\cite{pothier07:scalable_omniscient_debugging}.
While this system is very efficient, it has high requirements in terms of physical resources and set-up.
Later, Pothier and Tanter developed an approach for summarizing and indexing traces that supports querying arbitrarily large traces efficiently~\cite{pothier11:summarized_trace_indexing}.
Partial re-execution of program parts is used to retrieve information that was discarded to save memory.
A similar approach was used by Perscheid to achieve efficient back-in-time debugging with the Path tool suite~\cite{perscheid10:immediacy_through_interactivity_onlinea, perscheid13:test-driven_fault_navigation}.

Lienhard \etal developed a system for back-in-time debugging that reduces memory overhead by using the underlying VM's garbage collector~\cite{lienhard08:practical_object-oriented_back-in-time_debugging}.
They extended the Squeak Smalltalk VM to store object history along with the regular object.
When an object is no longer reachable, its history is considered irrelevant and will be discarded, too.

TARDIS by Barr and Marron is a back-in-time debugger for the .net runtime~\cite{barr14:tardis_affordable_time-travel_debugging}.
TARDIS uses a combination of snapshots and logging and is integrated in the runtime environment, thereby achieving very low run-time and memory overhead.
A similar debugger was developed for Microsoftâ€™s open-source ChakraCore JavaScript engine and the Node.js application framework~\cite{barr16:time-travel_debugging_for_javascriptnode}.
The debugger supports both live debugging with snapshots and post-mortem debugging from logs.

RR is a debugger that can record and replay arbitrary programs in Linux without modifications to the program code, the compilation pipeline, the runtime environment, or the operating system~\cite{ocallahan17:engineering_record_and_replay}.
This allows RR to be used as a general purpose debugger with low set-up and maintenance costs.

\section{Dynamic Slicing Algorithms}
\label{sec:rw_dynamic_slicing}

A history of different slicing approaches, since its inception by Weiser~\cite{weiser81:program_slicing}, was already presented in \cref{sec:evolution_of_slicing}, "\nameref{sec:evolution_of_slicing}".
In this section, we present a selection of tools and algorithms that are related to our use of slicing.

Ottenstein and Ottenstein proposed to represent programs as \acfp{pdg}~\cite{ottenstein84:the_program_dependence_graph}.
This representation is optimized for program analysis and can be re-used, reducing the time needed for subsequent analyses on the same code base.
Furthermore, they showed that slicing algorithms using \acp{pdg} have higher accuracy than other algorithms at the time and can run in linear time.

Korel and Laski created the distinction between static and dynamic slicing, the latter considering only one specific program input~\cite{korel88:dynamic_program_slicing}.
They presented an algorithm that computes executable dynamic slices.

Agrawal and Horgan used a different approach to compute dynamic slices~\cite{agrawal90:dynamic_program_slicing}.
They presented multiple algorithms based on execution traces and \acfp{ddg} that produces slices with higher accuracy, but are not guaranteed to be executable.

SLICE is a tool for computing dynmic slices of C programs~\cite{venkatesh95:experimental_results_from_dynamic}.
It allows users to choose between different algorithms to compute data, data and control, and executable slices.

\cite{venkatesh91:the_semantic_approach}!!!

Hall proposed simultaneous dynamic slicing, a technique to compute dynamic slices for multiple inputs~\cite{hall95:automatic_extraction_of_executable}.
Hall describes an abstract algorithm that can be applied in any slicing framework satisfying a few "mild" assumptions.
In a similar fashion, we introduced configurable slicing independent of concrete slicing approaches.

JSlice is a slicing tool for Java that creates dynamic slices from bytecode traces~\cite{wang08:dynamic_slicing_on_java}.

An in-depth survey and comparison of slicing tools was conducted by Hoffner in 1995~\cite{hoffner95:evaluation_and_comparison}.
In 1998, Korel and Rilling formally compared slicing approaches and algorithms~\cite{korel98:dynamic_program_slicing_methods}.
A more recent survey by Wong \etal on automatic fault localization also contains more recent developments in slicing~\cite{wong16:a_survey_on_software}.

\section{Slicing-based Debugging}
\label{sec:rw_slice_debugging}

Slicing can be used for many purposes, such as code optimization or security analysis.
When slicing is to be used to improve interactive debugging, it is not enough to just develop a slicing algorithm.
The slicer also has to be integrated in a tool that facilitates debugging the slice.

SPYDER is a back-in-time debugger for C with slicing capabilities~\cite{agrawal93:debugging_with_dynamic_slicing}.
Users can chose between data, control, and full slices and multiple algorithms with different performance/accuracy trade-offs.
The slices are not executable, but can be debugged on top of a full program execution.
The debugger uses checkpoints to facilitate execution backtracking.

Whyline lets developers ask why and why-not questions, such as "Why was this line not executed?"~\cite{ko08:debugging_reinvented_asking}.
Techniques related to dynamic and static slices are used to identify code locations that can answer the question.
Special integration with UI framework allows developers to ask question directly about UI components.

The Path tool suite and the Test-Driven Fault Navigation approach support a multitude of debugging techniques for reproducible failures in automated test cases~\cite{perscheid13:test-driven_fault_navigation}.
Tracing overhead is distributed over multiple runs, with incomplete trace being refined as needed.
Coverage and run-time analysis is used to identify suspicious code and corrupted state.

Using the Traceglasses~\cite{sakurai10:traceglasses_a_trace-based_debugger} trace-based back-in-time debugger, the Omission Finder user pointer assignment graphs and control flow graphs to locate omission bugs~\cite{sakurai15:the_omission_finder}.

\section{System Debuggers}
\label{sec:rw_system_debugging}

Our approach requires that searchable traces are available for all sub-systems of an application.
Solutions for collecting and storing execution traces always depend on the underlying technology, nevertheless some general approaches exist.

Mellor-Crummey and LeBlanc showed how to obtain instruction counters in software when they are not provided by the system~\cite{mellor-crummey89:a_software_instruction_counter}.
Their solution imposes a 10\% overhead and can be used to implement run-time analysis and reverse execution.

RR uses a generic approach for recording and replaying program executions~\cite{ocallahan_engineering_2017}.
Deterministic replay is achieved by recording system calls and no modifications of the operating system or the program's compiler are required.

Many approaches were developed to support debugging of distributed systems.
However, most of these systems require technological homogeneity, if not the same program running on every node.

Friday is a replay-based debugger for distributed applications that extends the GDB debugger and liblog replay library with distributed watchpoints, distributed breakpoints, and actions on distributed state~\cite{geels07:friday_global_comprehension}.

%With PQL, developers can search for the occurrence of code examples in program executions~\cite{martin05:finding_application_errors}.

D3S is a checking framework that can detect runtime errors in distributed systems~\cite{liu08:d3s_debugging_deployed_distributed}.

Recon is a debugging tool that allows developers to query the execution of distributed systems with SQL-like queries~\cite{lee11:unified_debugging_of_distributed}.
Queries are evaluated at run-time, with instrumentation code being compiled into the program as needed. 
Recon's query interface can provide all the information a debugger would need.


Magpie [5], Pinpoint [6], and Pip [23] are projects based
on log mining. In other words, they try to identify problems
by looking at event logs. These techniques are quite effective
in debugging performance problems, but less so for faults. In


[29], Xin et al. present a technique to analyze distributed systems
by building task graphs from event log files. 

Logging
and replay [7], [9], [26], [21] is an important strategy
for pervasive debugging. Existing work focuses on single
node replay, which is insufficient for pervasive debugging.





\section{Visualizations for Program Comprehension}
\label{sec:rw_visualization}

\cite{olsson91:sequential_debugging_at}
Dalek, debugger with hierarchy events, events extracted from sequential code.


Lanza showed the importance of visualizations in software development~\cite{lanza2003program}.
SHriMP views are one example how such visualizations can guide developers~\cite{storey2002shrimp}.
Baecker et al. used visualizations to improve the debugging process by animating algorithms~\cite{baecker1997software}.
Moher~\cite{moher1988provide} and Beguelin et al.~\cite{beguelin1993visualization} used visualizations to support the understanding of process interactions in heterogeneous environments.
However, in both works the focus was set more on understanding and debugging applications on an architectural level, while we aim to integrate debugging of individual components as if the software was a single program.


\chapter{Conclusion}
\label{sec:conclusion}

\section{Towards Full-System Debuggers}


\section{Future Work}

\newpage
x
\newpage
x
\newpage
x
\newpage
x
